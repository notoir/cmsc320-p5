{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "As technology develops and the world becomes more interconnected, there is an increasing need to protect personal information. The prevalence of the internet and digital bookkeeping introduces opportunities for malicious parties to acquire said information. One common motivator of data breaches is financial gain, and therefore credit card fraud and identity theft have become commonplace. In 2014 alone, there were 32 lost or stolen data records every second. North America is the most affected region of the world, accounting for almost three quarters of all breaches globally. These breaches affect Europe, Asia, and other parts of the world as well, as many transactions occur between parties in North America and elsewhere in the world [6].\n",
    "\n",
    "The data set we will be exploring is a large quantity of European card transactions in September 2013. There were approximately 285,000 transactions, and approximately 500 frauds. This number is still significant, as the proportion of breaches does not necessarily represent the damage caused by said breaches. Due to the sensitive nature of financial transactions, the features tracked in this data set are confidential. But, we can still use data science to determine patterns and relationships in the data, providing insight and predictive power into credit card fraud [1].\n",
    "\n",
    "This tutorial will cover principal component analysis (this is how the data is given, more information in the following sections), support vector machines, K nearest neighbors, random forests, and decision trees.\n",
    "\n",
    "You will need:\n",
    "- *Python 3 or higher*\n",
    "- *An internet connection*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[Installing libraries](#installing_libraries)\n",
    "\n",
    "\n",
    "[Principal Component Analysis](#pca)\n",
    "\n",
    "\n",
    "[Support Vector Machines](#svm)\n",
    "\n",
    "\n",
    "[KNN (K-Nearest Neighbors)](#knn)\n",
    "\n",
    "\n",
    "[Random Forests](#rf)\n",
    "\n",
    "\n",
    "[Cross Validation and Performance Analysis](#cv_pa)\n",
    "\n",
    "\n",
    "[Specifying Additional Hyperparameters](#hp)\n",
    "\n",
    "\n",
    "[Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by downloading the data set from the Kaggle web page [1].\n",
    "\n",
    "\n",
    "Unzip the creditcard.csv file into the same directory as the .py file you will be using to execute commands throughout\n",
    "the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='installing_libraries'></a>\n",
    "# Installing libraries\n",
    "Before we perform any transformations or operations on our data set, there are several prerequisite libraries that need to\n",
    "be installed. Arguably the most important library we will utilize is pandas, a Python data analysis toolkit [3]. This library\n",
    "allows us to parse large data sets into data frame objects, which are easily manipulated for comprehensive data\n",
    "transformations. Assuming you already have Python installed, you should install the Anaconda package manager. It will likely install more libraries than you require, but it will make the configuration process for executing commands in the tutorial vastly easier. Install Anaconda from [15], selecting the appropriate operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have installed Anaconda, let us begin by parsing in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load the CSV file into a pandas data frame object\n",
    "ccf_df = pd.read_csv(\"creditcard.csv\")\n",
    "# preview the first few rows of the data frame\n",
    "ccf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pandas method .head() to explore the first few rows of the data set, we can see that all of the values are\n",
    "fairly close to zero. This is because this data set is provided after a principal component analysis has been performed.\n",
    "\n",
    "<a id='pca'></a>\n",
    "# Principal Component Analysis\n",
    "Principal component analysis, in essence, realigns the axes of the data in order to account for the maximum amount of variance\n",
    "in the data as possible. The end result is that the data is projected onto a new set of axes, each of which is a linear\n",
    "combination of axes from the original data set. \n",
    "\n",
    "The benefit of PCA is that we can reduce the number of axes required to convey the information from the original data set. So, after performing PCA, we will end up with less features than we had previously, but the features will be more meaningful. If we have less features, there are less variables to find relationships between. And if there are less variables to find relationships between, program runtime will be greatly reduced, which can potentially save lots of time if the data set is large enough. See it visualized in [2].\n",
    "\n",
    "However, PCA has its downsides. By reducing the number of features, we are potentially losing out on precision in the data.\n",
    "Because of this, we are losing insight into what drives the model's predictive power. So, all results from this tutorial\n",
    "should be taken with a grain of salt.\n",
    "\n",
    "In order to better understand how principal component analysis works, look at the following image [4]: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/HmuvvRb.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the original axes are defined as:\n",
    "\n",
    "\n",
    "x: [-8, 10]\n",
    "\n",
    "\n",
    "y: [-6, 12]\n",
    "\n",
    "\n",
    "After performing PCA, the axes are refitted to the data. So, in our credit card fraud data set, the features V1 to V28 are values which are all relative to the newly realigned axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>\n",
    "# Support Vector Machines\n",
    "A support vector machine is a method for binary classification of data. Generally, it is used for determining whether a data point is of type A, or of type B. In a SVM, there is a defined boundary at which point data is classified. Depending on the set up, this boundary can take many shapes. At its simplest, it is a linear hyperplane. At its most complex, it may not be a continuous surface. The upside of using SVM is that it has a large amount of flexibility due to the variety of kernels and tuneable parameters, making it capable of modeling many types of data sets. The downside of using SVM is that it can be quite expensive to train with a high number of examples.\n",
    "\n",
    "If you would like to learn more about support vector machine, [5] is a good resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing the requisite libraries and preparing the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# we don't care about the time or amount column, they are not features we are training on\n",
    "ccf_df = ccf_df.drop('Time', 1)\n",
    "ccf_df = ccf_df.drop('Amount', 1)\n",
    "# we extract the labels and ditch the column in the data frame\n",
    "labels = ccf_df['Class']\n",
    "ccf_df = ccf_df.drop('Class', 1)\n",
    "# create variables for convenience sake when plugging parameters into the SVM function built into sklearn\n",
    "X = ccf_df.as_matrix()\n",
    "Y = labels.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the SVC and fit the data to it\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "# KNN (K Nearest Neighbors)\n",
    "KNN is a conceptually simple algorithm which uses a multi-dimensional geometric measure of fitness. Specifically, it approximates the similarity of two different observations as the geometric (Euclidean) distance between them when they are interpreted as vectors. It also makes the assumption that different classes of examples will tend to cluster in large groups, i.e. they all have similarity in certain dimensions that causes their distances to decrease. \n",
    "\n",
    "KNN is at its most useful when applied to data where one can expect all the data sharing a label to more or less look the same. It also tends to run very quickly and can easily support multiple labels. \n",
    "\n",
    "The disadvantages of KNN are that it does not deal well with data sets where labeled items can differ greatly from one another, or when observations with different labels are very close. Also, it tends to fare poorly in data sets with a high dimensionality. This is because, for large amounts of dimensions, the vast majority aren't going to differ between examples, meaning that the difference in their distance caused by meaningful features will be minimal. This is sometimes referred to as the \"curse of dimensionality\", for which more details can be found in [13].\n",
    "\n",
    "More information on KNN can be found in [11]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data frame is already established from the previous block of code used in the SVM section, we can run the following lines of code to train the KNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(15, weights='uniform')\n",
    "knn.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rf'></a>\n",
    "# Random Forests\n",
    "Random forests are a type of classifier that provide improved performance in both training time and accuracy to decision trees. Random forests are essentially an ensemble model of decision trees. An ensemble model is a model where many primitive models are trained together a single data set or differente subset/permutations of the same data set. Then, for each novel example, all the primitive models are asked to predict a label, and then the ensemble model takes an aggregate of their predictions. On large data sets, decision trees tend to overgeneralize on the first couple of levels, providing very little information at high computational cost. Random forests help to alleviate this problem, because each decision tree is trained with a small subset of the original data set. In addition, it is more likely to be able to draw conclusions about features that occur only a small number of times when that number is small compared to its slice of the data set. Finally, random forests can better deal with degenerate data sets as the different decision trees can select subsets that do no exist with the same general qualities.\n",
    "\n",
    "More detail on random forests can be found in [12]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before when training the KNN model, we can reuse the data frame created and tweaked in the SVM section of the tutorial. The following code will train a random forests model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(X, Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv_pa'></a>\n",
    "# Cross Validation and Performance Analysis\n",
    "Now that we have covered several different models for training data, we will begin to test them and evaluate which model has the best performance with the provided credit card fraud data set. In other words, we will determine which model is the best predictor of whether or not an entry in the data set is credit card fraud.\n",
    "\n",
    "In an effort to make comparisons among model performance clearer, I have developed a class which allows users to run all of the models covered in this tutorial consecutively, outputting their accuracy each time. Accuracy is computed via the method recommended by the poster of the data set [1], which is the area under the precision-recall curve. To read more about this method, check out [7].\n",
    "\n",
    "In addition to allowing the user to train multiple models consecutively, the class is also configurable to train the models using different combinations of hyperparameters. This feature is useful because users can investigate whether using different subsets of hyperparameters increases the runtime performance, accuracy, or both. By iterating through a list, the class will train the model using all of the provided hyperparameters in all possible configurations to determine which configuration works best.\n",
    "\n",
    "Below is the code for the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# sample input:\n",
    "# {'a': [1,2], 'b': [3,4]}\n",
    "# sample output:\n",
    "# {'a': 1, 'b': 3} {'a': 1, 'b': 4} {'a': 2, 'b': 3} {'a': 2, 'b': 4}\n",
    "def get_permutations(hp_dict):\n",
    "    keys = list(hp_dict.keys())\n",
    "    vals = [hp_dict[k] for k in keys] # list of lists\n",
    "    for perm in itertools.product(*vals):\n",
    "        print(perm)\n",
    "        yield dict(zip(keys, perm))\n",
    "\n",
    "class ClassificationModel(object):\n",
    "    \"\"\"Base class for models within this framework. Anything that can be trained on labeled examples \n",
    "    then predict unseen examples can become a subclass of this.\n",
    "    \n",
    "    Subclasses must implement _train, _predict, id, and default_hyperparameters at a minimum.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.hparams = self.default_hyperparameters()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '(' + self.id() + '-' + '-'.join(['-' + self.hparams[k] for k in self.hparams]) + ')'\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Trains the model on all possible permutations of its given hyperparameters. Subclass-specific \n",
    "        training with one set of hyperparameters occurs in the _train() method.\"\"\"\n",
    "        self.results = {}\n",
    "        for perm in get_permutations(self.hparams):\n",
    "            print(\"Training permutation {} of {}\".format(perm, self.id()))\n",
    "            tmp = tuple(perm.items())\n",
    "            self.results[tmp] = self._train(X, y, perm)\n",
    "            print(\"...done\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts the labels on the given examples using all the models trained in the train() method.\n",
    "        Subclass-specific prediction code is done in _predict().\"\"\"\n",
    "        self.predictions = {}\n",
    "        for (hp, obj) in self.results.items():\n",
    "            print(\"Predicting permutation {} for {}\".format(hp, self.id()))\n",
    "            self.predictions[hp] = self._predict(X, obj)\n",
    "            print(\"...done\")\n",
    "            \n",
    "        return self.predictions\n",
    "\n",
    "    def id(self):\n",
    "        \"\"\"A short string identifier, unique to each model, that distinguishes it for the purpose of \n",
    "        organizing results and string representations.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _train(self, X, y, hp):\n",
    "        \"\"\"Trains an instance of this model. The object returned by this function will be stored in a dictionary\n",
    "        and retrieved, when appropriate, for use by the _predict method.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): An MxN matrix of examples, where M is number of examples and N is number of features.\n",
    "            y (np.ndarray): An Mx1 vector of labels.\n",
    "            hp (Dict[string, Any]): Dictionary of hyperparameters. A single permutation from the cartesian product\n",
    "                of all hyperparameters given in self.hparams.\n",
    "        \n",
    "        Returns:\n",
    "            Any: The implementing class can return anything it likes (hopefully, something with state that occurs\n",
    "                as a result of training on the data!). This object will be available in _predict.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict(self, X, obj):\n",
    "        \"\"\"Predicts the labels for a set of unseen examples. Returns a vector of predicted labels.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray): An MxN matrix of examples. N is the same as in _train.\n",
    "            obj (Any): The object stored as a result of training in the _train method. Should contain state based \n",
    "                on training data.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: An Mx1 vector of labels. Can be 'probability' labels that express confidence in labeling the\n",
    "                example as a particular category.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def default_hyperparameters(self):\n",
    "        \"\"\"Returns a reasonable default set of hyperparameters for use as this object's self.hparams member.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[String, List[Any]]: A dictionary mapping hyperparameter names to (singleton) lists containing a\n",
    "                default value.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class KNNModel(ClassificationModel):\n",
    "    def id(self):\n",
    "        return 'knn'\n",
    "\n",
    "    def _train(self, X, y, hp):\n",
    "        knn = KNeighborsClassifier(**hp)\n",
    "        knn.fit(X, y)\n",
    "        return knn\n",
    "\n",
    "    def _predict(self, X, obj):\n",
    "        return obj.predict(X)\n",
    "\n",
    "    def default_hyperparameters(self):\n",
    "        return {\n",
    "                'n_neighbors': [30], # number of closest neighbors to consider\n",
    "                'p': [2], # dimension p of L_p norm (minkowski distance)\n",
    "                }\n",
    "\n",
    "class SVMModel(ClassificationModel):\n",
    "    def id(self):\n",
    "        return 'svm'\n",
    "\n",
    "    def _train(self, X, y, hp):\n",
    "        svm = SVC(**hp)\n",
    "        svm.fit(X, y)\n",
    "        return svm\n",
    "\n",
    "    def _predict(self, X, obj):\n",
    "        return obj.predict(X)\n",
    "\n",
    "    def default_hyperparameters(self):\n",
    "        return {\n",
    "                'kernel': ['rbf'], # kernel function: chosen from 'linear', 'rbf', 'sigmoid'\n",
    "                'shrinking': [True], # whether to use the shrinking heuristic\n",
    "                'C': [1.0],    # penalty parameter of error term\n",
    "                }\n",
    "\n",
    "class RandomForestModel(ClassificationModel):\n",
    "    def id(self):\n",
    "        return 'random_forest'\n",
    "\n",
    "    def _train(self, X, y, hp):\n",
    "        rf = RandomForestClassifier(**hp)\n",
    "        rf.fit(X, y)\n",
    "        return rf\n",
    "\n",
    "    def _predict(self, X, obj):\n",
    "        return obj.predict(X)\n",
    "\n",
    "    def default_hyperparameters(self):\n",
    "        return {\n",
    "                'n_estimators': [10], # number of trees in the forest\n",
    "                'oob_score': [False], # whether to use out-of-bag samples\n",
    "                'max_features': ['sqrt'], # number of features to consider at each split\n",
    "                # can also be an int, a float (percentage), 'log2' or None\n",
    "                }\n",
    "\n",
    "class ClassificationProblem(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.models = []\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "    def cross_validation(self, folds=10):\n",
    "        \"\"\"Runs n-fold cross-validation on a model using all datasets provided by the user.\n",
    "        \n",
    "        Args:\n",
    "            folds (int)=10: The number n of \"folds\" to use. Essentially, split the data into n equal \n",
    "                parts and use one for training, and the rest for testing. This is because we only have\n",
    "                access to the training data.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[Tuple[int, str], Dict[Tuple[str, Any], float]]: Results of the cross-validation. For\n",
    "                each unique combination of fold index and model id, returns the accuracy of each set of\n",
    "                hyperparameters for that model and fold.\"\"\"\n",
    "        # shuffling the array prevents any inherent bias in the dataset (all positive examples coming first, \n",
    "        # for example).\n",
    "        np.random.shuffle(self.X)\n",
    "        sample_size = int(self.X.shape[0] / folds)\n",
    "        accuracy = {}\n",
    "        for i in range(folds):\n",
    "            print(\"Doing slice {} of {}\".format(i, folds))\n",
    "            start = i*sample_size\n",
    "            end = (i+1)*sample_size\n",
    "            test_slice = (self.X[start:end], self.y[start:end])\n",
    "            # boolean mask indicating membership in training set\n",
    "            indices = np.in1d(np.arange(len(self.X)), np.r_[0:start,end:len(self.X)])\n",
    "            train_slice = (self.X[indices], self.y[indices]))\n",
    "            for m in self.models:\n",
    "                m.train(*train_slice)\n",
    "                predictions = m.predict(test_slice[0])\n",
    "                accuracy[(i, m.id())] = {k: average_precision_score(test_slice[1], v) for k, v in predictions.items()}\n",
    "                \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a ClassificationModel object and add the desired classes to it. Then, perform a cross validaton and output the results.\n",
    "\n",
    "Note that the execution will drastically increase as you add more hyperparameters to the model, as it runs all possible combinations of hyperparameters. Also note that execution will vary depending on your hardware configuration. There are almost three hundred-thousand entries in the data set, so be prepared for a lengthy execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the object\n",
    "cp = ClassificationProblem(X, y)\n",
    "# initialize the desired models\n",
    "knn = KNNModel()\n",
    "svm = SVMModel()\n",
    "rf = RandomForestModel()\n",
    "# add the models to the CP object\n",
    "cp.add_model(knn)\n",
    "cp.add_model(svm)\n",
    "cp.add_model(rf)\n",
    "# perform a cross-validation test\n",
    "acc = cp.cross_validation(3)\n",
    "# output the accuracy\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran this code on my desktop computer, which has a has a quad-core processor. It took roughly forty-five minutes to complete. \n",
    "\n",
    "This was the output (I added new lines and spaces for clarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{(0, 'svm'): {(('kernel', 'rbf'), ('shrinking', True), ('C', 1.0)): 0.50114288723863698},\n",
    " (0, 'knn'): {(('n_neighbors', 30), ('p', 2)): 0.50114288723863698},\n",
    " (0, 'random_forest'): {(('n_estimators', 10), ('oob_score', False), ('max_features', 'sqrt')): 0.0011428872386369622},\n",
    " \n",
    " (1, 'svm'): {(('kernel', 'rbf'), ('shrinking', True), ('C', 1.0)): 0.50080581450466111},\n",
    " (1, 'knn'): {(('n_neighbors', 30), ('p', 2)): 0.50080581450466111},\n",
    " (1, 'random_forest'): {(('n_estimators', 10), ('oob_score', False), ('max_features', 'sqrt')): 0.00080581450466108394},\n",
    " \n",
    " (2, 'svm'): {(('kernel', 'rbf'), ('shrinking', True), ('C', 1.0)): 0.50064254489914151},\n",
    " (2, 'knn'): {(('n_neighbors', 30), ('p', 2)): 0.50064254489914151},\n",
    " (2, 'random_forest'): {(('n_estimators', 10), ('oob_score', False), ('max_features', 'sqrt')): 0.040449924507696564}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN with with 30 neighbors and SVM performed identically, which both vastly outperformed random forests on this data set. This means that KNN and SVM were both equally as effective models for predicting fraud.\n",
    "\n",
    "A slightly higher than .5 AUPRC value indicates that the model is doing slightly better than guessing randomly. For more information on the area under precision-recall curve, check out [14]. By this same metric, random forests are performing drastically worse than randomly guessing, so they are not a useful model in this situation.\n",
    "\n",
    "If a model guesses In this situation, the trade off between high training time and low prediction time for SVM and low training time and high prediction time for KNN are equally beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hp'></a>\n",
    "# Specifying Additional Hyperparameters\n",
    "Suppose you want to determine the accuracy of a KNN model with 30 neighbors, but also 50 neighbors. To do this, append 50 to the list of parameters in the 'default_hyperparamters' method of the KNN model in the class code. The model will train with both 30 and 50 neighbors, so you can compare performance. The same method can be applied to any model and any parameter. You can also add hyperparameters not specified in the 'default_hyperparameters' method by looking through all available optional parameters in the scikit-learn documentation, and adding it to the existing dictionary of hyperparameters.\n",
    "\n",
    "To test this, we will try out 30 estimators as well as 'log2' as a max_feature paramters. The following code should be used for the default_hyperparameters function in the RandomForestModel class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def default_hyperparameters(self):\n",
    "        return {\n",
    "                'n_estimators': [10, 30], # number of trees in the forest\n",
    "                'oob_score': [False], # whether to use out-of-bag samples\n",
    "                'max_features': ['sqrt', 'log2'], # number of features to consider at each split\n",
    "                # can also be an int, a float (percentage), 'log2' or None\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Running the code with just a RandomForestModel yields the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{(0, 'random_forest'): {(('n_estimators', 10), ('oob_score', False), ('max_features', 'sqrt')): 0.0011428872386369622, \n",
    "                        (('n_estimators', 10), ('oob_score', False), ('max_features', 'log2')): 0.0011428872386369622, \n",
    "                        (('n_estimators', 30), ('oob_score', False), ('max_features', 'sqrt')): 0.0011428872386369622, \n",
    "                        (('n_estimators', 30), ('oob_score', False), ('max_features', 'log2')): 0.0011428872386369622}, \n",
    " (1, 'random_forest'): {(('n_estimators', 10), ('oob_score', False), ('max_features', 'sqrt')): 0.00080581450466108394, \n",
    "                        (('n_estimators', 10), ('oob_score', False), ('max_features', 'log2')): 0.00080581450466108394, \n",
    "                        (('n_estimators', 30), ('oob_score', False), ('max_features', 'sqrt')): 0.00080581450466108394, \n",
    "                        (('n_estimators', 30), ('oob_score', False), ('max_features', 'log2')): 0.00080581450466108394}, \n",
    " (2, 'random_forest'): {(('n_estimators', 10), ('oob_score', False), ('max_features', 'sqrt')): 0.00064254489914151785, \n",
    "                        (('n_estimators', 10), ('oob_score', False), ('max_features', 'log2')): 0.00064254489914151785, \n",
    "                        (('n_estimators', 30), ('oob_score', False), ('max_features', 'sqrt')): 0.00064254489914151785, \n",
    "                        (('n_estimators', 30), ('oob_score', False), ('max_features', 'log2')): 0.00064254489914151785}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the description for each hyperparameter left in the comments is not helpful, the scikit-learn documentation for the model in question has longer and more comprehensive descriptions. The documentation for those models can be found at [8], [9], and [10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Ideally, after reading this tutorial, you should be familiar with the advantages and disadvantages of the three listed machine learning models. In addition, you should also have gained an understanding of why each model trains and predicts the way it does. While the features of this data set remain confidential, and thus, so does the context in which credit card fraud occurs, we can still use them with machine learning to extrapolate patterns and predict whether or not fraud has been committed. So, these models can be provided to concerned parties who can use them to predict fraud without the user ever knowing the intimate details about the features in the data set that were hidden up front."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sources'></a>\n",
    "# Sources\n",
    "[1] The data set: https://www.kaggle.com/dalpozz/creditcardfraud\n",
    "\n",
    "\n",
    "[2] Principal component analysis (visualized): http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "\n",
    "[3] Pandas: http://pandas.pydata.org/\n",
    "\n",
    "\n",
    "[4] PCA example: https://upload.wikimedia.org/wikipedia/commons/f/f5/GaussianScatterPCA.svg\n",
    "\n",
    "\n",
    "[5] SVM slides: https://www.cise.ufl.edu/class/cis4930sp11dtm/notes/intro_svm_new.pdf\n",
    "\n",
    "[6] Credit card fraud statistics:\n",
    "\n",
    "\n",
    "http://www.creditcards.com/credit-card-news/credit-card-security-id-theft-fraud-statistics-1276.php\n",
    "\n",
    "[7] AUCPR paper: http://pages.cs.wisc.edu/~boyd/aucpr_final.pdf\n",
    "\n",
    "[8] SVM scikit-learn documentation: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "\n",
    "[9] Random Forest Classifier: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "[10] KNN: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "[11] KNN, in detail: http://www.statsoft.com/Textbook/k-Nearest-Neighbors\n",
    "\n",
    "[12] Random Forests Paper: https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf\n",
    "\n",
    "[13] \"Curse of Dimensionality\": \n",
    "http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/\n",
    "\n",
    "[14] Area Under Precision-Recall Curve article:\n",
    "http://fastml.com/what-you-wanted-to-know-about-auc/\n",
    "\n",
    "[15] Anaconda package manager: https://www.continuum.io/downloads\n",
    "\n",
    "Written by Mark Newton."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
